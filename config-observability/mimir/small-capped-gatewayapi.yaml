mimirGateway:
  enabled: true
  parentGatewayName: "eg-observability"
  parentGatewayNamespace: "observability"
  
  # Leave empty or comment out to make hostnames optional (catch-all)
  hostnames: 
    - "metrics.posindonesia.deploy-at.my.id"
  
  # Custom path prefix
  path: "/"
  
  servicePort: 80
mimir-distributed:
  # Enabled mimir ingress for center of the architecture
  ingress:
    enabled: false

  # disable minio for object storage
  minio:
    enabled: false
    
  global:
    extraEnv:
      - name: MY_POD_IP
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
  # Global Config: Reduce replication to save 33% of resources
  mimir:
    structuredConfig:
      # This is the only required setting, it tells Mimir to use the gossip protocol for cluster membership and replication
      memberlist:
        advertise_addr: ${MY_POD_IP}
      common:
        storage:
          backend: s3
          s3:
            bucket_name: observability-k8s-pos-indonesia
            endpoint: s3.ap-southeast-3.amazonaws.com
            region: ap-southeast-3

      blocks_storage:
        s3:
          bucket_name: observability-k8s-pos-indonesia
          endpoint: s3.ap-southeast-3.amazonaws.com
          region: ap-southeast-3
        storage_prefix: tsdb

      alertmanager_storage:
        s3:
          bucket_name: observability-k8s-pos-indonesia
          endpoint: s3.ap-southeast-3.amazonaws.com
          region: ap-southeast-3
        storage_prefix: alertmanager

      ruler_storage:
        s3:
          bucket_name: observability-k8s-pos-indonesia
          endpoint: s3.ap-southeast-3.amazonaws.com
          region: ap-southeast-3
        storage_prefix: ruler
      ingest_storage:
        enabled: true
        kafka:
          producer_max_record_size_bytes: 15983616 # 20MB, adjust if you have larger samples or want to optimize for fewer/larger messages
      limits:
        ingestion_rate: 2000000 # Limit ingestion to 2M samples/second across the cluster
        ingestion_burst_size: 4000000 # Allow bursts of up to 4M samples
        # Limit queries to 500 days. You can override this on a per-tenant basis.
        max_total_query_length: 12000h
        # Adjust max query parallelism to 16x sharding, without sharding we can run 15d queries fully in parallel.
        # With sharding we can further shard each day another 16 times. 15 days * 16 shards = 240 subqueries.
        max_query_parallelism: 240
        # Avoid caching results newer than 10m because some samples can be delayed
        # This presents caching incomplete results
        max_cache_freshness: 10m
        # Maximum number of rules per rule group per-tenant. 0 to disable.
        ruler_max_rules_per_rule_group: 0
        # Maximum number of rule groups per-tenant. 0 to disable.
        ruler_max_rule_groups_per_tenant: 0  
        max_global_series_per_user: 0 # 0 means no limit, set to a reasonable number if you want to prevent a single tenant from consuming all resources

  kafka:
    enabled: true
    replicas: 3
    persistence:
      enabled: false # Kafka is only used for replication in this setup, so we can disable persistence to save resources, use emptyDir for storage
      storageClassName: "gp3-automode-eks-dev"
    nodeSelector:
      karpenter.sh/capacity-type: on-demand
    resources:
      requests:
        cpu: 1
        memory: 1Gi
      limits: 
        memory: 1Gi 

  alertmanager:
    replicas: 1  # Minimalist (Single pod is enough for basic alerting)
    persistentVolume:
      storageClass: "gp3-automode-eks-dev"
    nodeSelector:
      karpenter.sh/capacity-type: on-demand
    resources:
      requests:
        cpu: 100m
        memory: 256Mi

  compactor:
    persistentVolume:
      size: 30Gi # Reduced from 50Gi
      storageClass: "gp3-automode-eks-dev"
    nodeSelector:
      karpenter.sh/capacity-type: on-demand
    resources:
      requests:
        cpu: 500m
        memory: 1Gi # Essential for 1-year retention processing

  distributor:
    replicas: 2
    nodeSelector:
      karpenter.sh/capacity-type: spot
    resources:
      requests:
        cpu: 200m
        memory: 512Mi # Distributors are stateless and light

  ingester:
    replicas: 1  # Reduced from 3 (Matches RF=2)
    persistentVolume:
      size: 20Gi # Reduced (We flush to S3 every 2h anyway)
      storageClass: "gp3-automode-eks-dev"
    nodeSelector:
      karpenter.sh/capacity-type: on-demand
    resources:
      requests:
        cpu: 500m
        memory: 2Gi # Minimum safe RAM for 1M active series
    zoneAwareReplication:
      enabled: false  # Disable for small setups to save resources

  store_gateway:
    replicas: 1  # Matches RF=2
    persistentVolume:
      size: 10Gi
      storageClass: "gp3-automode-eks-dev"
    nodeSelector:
      karpenter.sh/capacity-type: on-demand
    resources:
      requests:
        cpu: 200m
        memory: 1Gi # Low RAM thanks to lazy loading
    zoneAwareReplication:
      enabled: false  # Disable for small setups to save resources

  querier:
    replicas: 2 # 2 for basic HA on queries
    nodeSelector:
      karpenter.sh/capacity-type: spot
    resources:
      requests:
        cpu: 100m
        memory: 128Mi

  query_frontend:
    replicas: 1
    nodeSelector:
      karpenter.sh/capacity-type: spot
    resources:
      requests:
        cpu: 100m
        memory: 256Mi

  # Disable all extra caches to save cost/complexity unless query speed is slow
  chunks-cache: { enabled: false }
  index-cache: { enabled: false }
  metadata-cache: { enabled: false }
  results-cache: { enabled: false }

  gateway:
    replicas: 1
    nodeSelector:
      karpenter.sh/capacity-type: on-demand
    resources:
      requests:
        cpu: 100m
        memory: 128Mi